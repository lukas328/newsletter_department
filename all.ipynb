{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5d3f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 3)) (2.11.5)\n",
      "Requirement already satisfied: requests in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 4)) (2.32.3)\n",
      "Collecting langchain-google-genai (from -r requirements.txt (line 5))\n",
      "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->-r requirements.txt (line 3)) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->-r requirements.txt (line 3)) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->-r requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->-r requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->-r requirements.txt (line 4)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->-r requirements.txt (line 4)) (2025.4.26)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.62 (from langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading langchain_core-0.3.63-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading google_api_core-2.25.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5)) (2.40.2)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading protobuf-6.31.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting langsmith<0.4,>=0.1.126 (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading grpcio-1.72.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5)) (4.9.1)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.62->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai->-r requirements.txt (line 5)) (0.28.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading orjson-3.10.18-cp312-cp312-win_amd64.whl.metadata (43 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai->-r requirements.txt (line 5))\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-win_amd64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai->-r requirements.txt (line 5)) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai->-r requirements.txt (line 5)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai->-r requirements.txt (line 5)) (0.16.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->-r requirements.txt (line 5)) (0.6.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\lukasdech\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai->-r requirements.txt (line 5)) (1.3.1)\n",
      "Downloading langchain_google_genai-2.1.5-py3-none-any.whl (44 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.5/1.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.0/1.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.63-py3-none-any.whl (438 kB)\n",
      "Downloading google_api_core-2.25.0-py3-none-any.whl (160 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-6.31.1-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading grpcio-1.72.1-cp312-cp312-win_amd64.whl (4.2 MB)\n",
      "   ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/4.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.8/4.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.3/4.2 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 1.6/4.2 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 2.1/4.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.4/4.2 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.9/4.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.1/4.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.7/4.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.2/4.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.2/4.2 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading grpcio_status-1.72.1-py3-none-any.whl (14 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading orjson-3.10.18-cp312-cp312-win_amd64.whl (134 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading zstandard-0.23.0-cp312-cp312-win_amd64.whl (495 kB)\n",
      "Installing collected packages: filetype, zstandard, tenacity, PyYAML, protobuf, packaging, orjson, jsonpointer, grpcio, requests-toolbelt, proto-plus, jsonpatch, googleapis-common-protos, langsmith, grpcio-status, google-api-core, langchain-core, google-ai-generativelanguage, langchain-google-genai\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "Successfully installed PyYAML-6.0.2 filetype-1.2.0 google-ai-generativelanguage-0.6.18 google-api-core-2.25.0 googleapis-common-protos-1.70.0 grpcio-1.72.1 grpcio-status-1.72.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.63 langchain-google-genai-2.1.5 langsmith-0.3.45 orjson-3.10.18 packaging-24.2 proto-plus-1.26.1 protobuf-6.31.1 requests-toolbelt-1.0.0 tenacity-9.1.2 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script filetype.exe is installed in 'c:\\Users\\LukasDech\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b843488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\LukasDech\\OneDrive - mesoneer\\Documents\\newsletter_department\\src\\agents\\data_fetchers\\newsapi_fetcher.py\", line 11, in <module>\n",
      "    from models.data_models import RawArticle # Unser Pydantic-Modell für Rohartikel\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'models'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python src/agents/data_fetchers/newsapi_fetcher.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521ab1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c2767",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1506332577.py, line 263)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 263\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m```python\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "# Haupt-Einstiegspunkt zum Starten der Newsletter-Pipeline.\n",
    "\n",
    "from src.orchestrator import NewsletterOrchestrator\n",
    "from src.utils.logging_setup import setup_logging # Sicherstellen, dass Logging früh konfiguriert wird\n",
    "import logging # Importiere logging hier, um es direkt zu verwenden, falls nötig\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Grundlegendes Logging für den Fall, dass der Orchestrator nicht initialisiert wird\n",
    "    # oder Fehler vor dessen Logging-Setup auftreten.\n",
    "    # Das Logging wird im Orchestrator detaillierter konfiguriert.\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger(__name__) # Logger für main.py\n",
    "\n",
    "    logger.info(\"Starte den Newsletter-Generierungsprozess...\")\n",
    "    try:\n",
    "        orchestrator = NewsletterOrchestrator()\n",
    "        result_link = orchestrator.run_pipeline()\n",
    "        \n",
    "        if result_link:\n",
    "            logger.info(f\"Newsletter-Pipeline erfolgreich abgeschlossen. Newsletter verfügbar unter: {result_link}\")\n",
    "        else:\n",
    "            logger.warning(\"Newsletter-Pipeline abgeschlossen, aber es wurde kein Link zum Newsletter generiert (möglicherweise Fehler oder keine Daten).\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Ein kritischer, nicht abgefangener Fehler ist in main.py aufgetreten: {e}\", exc_info=True)\n",
    "        # exc_info=True fügt den Traceback zum Log hinzu.\n",
    "\n",
    "# src/orchestrator.py\n",
    "# Steuert den gesamten Ablauf der Newsletter-Generierung.\n",
    "\n",
    "import logging\n",
    "from datetime import datetime, timezone # timezone hinzugefügt\n",
    "from typing import List, Optional, Any, Dict # Dict hinzugefügt\n",
    "import os\n",
    "\n",
    "from src.utils.config_loader import load_env, get_env_variable # get_env_variable hinzugefügt\n",
    "from src.utils.logging_setup import setup_logging\n",
    "\n",
    "# Datenbeschaffer\n",
    "from src.agents.data_fetchers.newsapi_fetcher import NewsAPIFetcher\n",
    "# from src.agents.data_fetchers.rss_fetcher import RSSFetcher # Beispiel für weitere Fetcher\n",
    "# from src.agents.data_fetchers.google_calendar_fetcher import GoogleCalendarEventFetcher, GoogleCalendarBirthdayFetcher # Beispiel\n",
    "# from src.agents.data_fetchers.weather_fetcher import OpenWeatherMapFetcher # Beispiel\n",
    "\n",
    "# LLM-Prozessoren\n",
    "from src.agents.llm_processors.summarizer_agent import SummarizerAgent\n",
    "from src.agents.llm_processors.categorizer_agent import CategorizerAgent\n",
    "from src.agents.llm_processors.evaluator_agent import AudienceAlignmentAgent # NEUER AGENT\n",
    "\n",
    "# Datenmodelle\n",
    "from src.models.data_models import (\n",
    "    RawArticle, ProcessedArticle, Event, Birthday, WeatherInfo, \n",
    "    NewsletterData, NewsletterSection\n",
    ")\n",
    "\n",
    "# Newsletter-Erstellung und Verteilung\n",
    "from src.agents.newsletter_composer import NewsletterComposer\n",
    "from src.agents.distributors.google_drive_uploader import GoogleDriveUploader\n",
    "\n",
    "\n",
    "# LangSmith Konfiguration (wird automatisch von LangChain genutzt, wenn Umgebungsvariablen gesetzt sind)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NewsletterOrchestrator:\n",
    "    def __init__(self):\n",
    "        load_env()\n",
    "        # Hole LOG_LEVEL und LOG_FILE aus .env, mit Defaults\n",
    "        log_level = get_env_variable(\"LOG_LEVEL\", \"INFO\")\n",
    "        log_file_path = get_env_variable(\"LOG_FILE\") # Kann None sein\n",
    "        setup_logging(log_level_str=log_level, log_file=log_file_path)\n",
    "        logger.info(\"Initialisiere Newsletter Orchestrator...\")\n",
    "\n",
    "        self.news_fetchers = [\n",
    "            NewsAPIFetcher(query=\"international OR world OR global\", endpoint=\"everything\", days_ago=1, source_name_override=\"Weltnachrichten (NewsAPI)\"),\n",
    "            NewsAPIFetcher(query=\"Technologie OR KI OR Innovation OR Wissenschaft\", endpoint=\"everything\", days_ago=1, source_name_override=\"Technologie & Wissenschaft (NewsAPI)\"),\n",
    "            NewsAPIFetcher(query=\"Wirtschaft OR Finanzen OR Börse\", endpoint=\"everything\", days_ago=1, source_name_override=\"Wirtschaft (NewsAPI)\"),\n",
    "            NewsAPIFetcher(query=\"Zürich OR Schweiz\", endpoint=\"top-headlines\", country=\"ch\", category=\"general\", source_name_override=\"Zürich/Schweiz Schlagzeilen (NewsAPI)\"),\n",
    "        ]\n",
    "        \n",
    "        # LLM-Prozessoren\n",
    "        self.summarizer = SummarizerAgent()\n",
    "        self.categorizer = CategorizerAgent(\n",
    "            categories=[ \n",
    "                \"IT & AI\", \"Welt und Politik\", \"Wirtschaft\", \n",
    "                \"Zürich Inside\", \"Kultur und Inspiration\", \"Der Rund um Blick\"\n",
    "            ]\n",
    "        )\n",
    "        # NEUER AGENT: AudienceAlignmentAgent\n",
    "        # Das user_profile sollte idealerweise aus einer Konfigurationsdatei oder einer separaten Datei geladen werden.\n",
    "        user_profile_text = get_env_variable(\"NEWSLETTER_USER_PROFILE\", \"\"\"\n",
    "Dies ist ein Default-Profil: Leser interessiert sich für allgemeine Technologie-Neuigkeiten und wichtige globale Ereignisse.\n",
    "Bevorzugt kurze, prägnante Zusammenfassungen. Weniger interessiert an Klatsch oder sehr nischigen Themen.\n",
    "\"\"\") # Lade Profil aus .env oder nutze Default\n",
    "        self.audience_aligner = AudienceAlignmentAgent(user_profile=user_profile_text)\n",
    "\n",
    "\n",
    "    def _fetch_all_raw_articles(self) -> List[RawArticle]:\n",
    "        all_raw_articles: List[RawArticle] = []\n",
    "        for fetcher in self.news_fetchers:\n",
    "            try:\n",
    "                articles = fetcher.fetch_data()\n",
    "                all_raw_articles.extend(articles)\n",
    "                logger.info(f\"{len(articles)} Artikel von '{fetcher.source_name}' abgerufen.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Fehler beim Abrufen von Artikeln von '{fetcher.source_name}': {e}\", exc_info=True)\n",
    "        return all_raw_articles\n",
    "\n",
    "    def _process_articles_initial(self, raw_articles: List[RawArticle]) -> List[ProcessedArticle]:\n",
    "        \"\"\"Erste Verarbeitungsstufe: Zusammenfassen und Kategorisieren.\"\"\"\n",
    "        initial_processed_articles: List[ProcessedArticle] = []\n",
    "        logger.info(f\"Starte initiale Verarbeitung von {len(raw_articles)} Roh-Artikeln...\")\n",
    "\n",
    "        articles_to_process = raw_articles[:20] # Begrenze für Testzwecke\n",
    "        logger.info(f\"Verarbeite die ersten {len(articles_to_process)} Artikel mit Summarizer und Categorizer...\")\n",
    "\n",
    "        for i, raw_article in enumerate(articles_to_process):\n",
    "            try:\n",
    "                logger.debug(f\"Initial verarbeite Artikel {i+1}/{len(articles_to_process)}: {raw_article.title}\")\n",
    "                summary = self.summarizer.summarize_article(raw_article)\n",
    "                category = self.categorizer.categorize_article(raw_article, summary)\n",
    "                \n",
    "                initial_processed_articles.append(\n",
    "                    ProcessedArticle(\n",
    "                        title=raw_article.title or \"Unbekannter Titel\",\n",
    "                        url=raw_article.url,\n",
    "                        summary=summary,\n",
    "                        category=category,\n",
    "                        source_name=raw_article.source_name,\n",
    "                        published_at=raw_article.published_at,\n",
    "                        llm_processing_details={\"initial_processing\": \"success\"}\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Fehler bei der initialen LLM-Verarbeitung für Artikel '{raw_article.title}': {e}\", exc_info=True)\n",
    "        \n",
    "        logger.info(f\"{len(initial_processed_articles)} Artikel initial mit LLMs verarbeitet.\")\n",
    "        return initial_processed_articles\n",
    "\n",
    "    def _evaluate_and_filter_articles(self, articles_to_evaluate: List[ProcessedArticle]) -> List[ProcessedArticle]:\n",
    "        \"\"\"Zweite Verarbeitungsstufe: Evaluierung und Filterung.\"\"\"\n",
    "        final_approved_articles: List[ProcessedArticle] = []\n",
    "        logger.info(f\"Starte Evaluierung von {len(articles_to_evaluate)} vorverarbeiteten Artikeln...\")\n",
    "\n",
    "        # Schwellenwert für Relevanz-Score, könnte aus Konfiguration kommen\n",
    "        relevance_threshold = float(get_env_variable(\"RELEVANCE_THRESHOLD\", \"0.5\")) \n",
    "\n",
    "        for i, article in enumerate(articles_to_evaluate):\n",
    "            try:\n",
    "                logger.debug(f\"Evaluiere Artikel {i+1}/{len(articles_to_evaluate)}: {article.title}\")\n",
    "                \n",
    "                evaluation_result = self.audience_aligner.evaluate_article(article)\n",
    "                \n",
    "                # Stelle sicher, dass llm_processing_details existiert\n",
    "                if article.llm_processing_details is None:\n",
    "                    article.llm_processing_details = {}\n",
    "                article.llm_processing_details[\"audience_evaluation\"] = evaluation_result\n",
    "                \n",
    "                # Aktualisiere den relevance_score im Artikelobjekt\n",
    "                article.relevance_score = evaluation_result.get(\"relevance_score\", 0.0)\n",
    "\n",
    "                logger.info(f\"Artikel '{article.title}' Audience-Score: {article.relevance_score:.2f}. Feedback: {evaluation_result.get('feedback_summary', 'Kein Feedback')}\")\n",
    "\n",
    "                if article.relevance_score >= relevance_threshold:\n",
    "                    final_approved_articles.append(article)\n",
    "                else:\n",
    "                    logger.info(f\"Artikel '{article.title}' aufgrund niedrigen Relevanz-Scores ({article.relevance_score:.2f} < {relevance_threshold}) nicht für Newsletter ausgewählt.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Fehler bei der Evaluierung von Artikel '{article.title}': {e}\", exc_info=True)\n",
    "        \n",
    "        logger.info(f\"{len(final_approved_articles)} Artikel nach Evaluierung für den Newsletter ausgewählt.\")\n",
    "        return final_approved_articles\n",
    "\n",
    "    def run_pipeline(self) -> Optional[str]:\n",
    "        logger.info(\"Newsletter-Generierungspipeline gestartet.\")\n",
    "        start_time = datetime.now(timezone.utc)\n",
    "\n",
    "        # 1. Daten sammeln\n",
    "        raw_articles = self._fetch_all_raw_articles()\n",
    "        if not raw_articles:\n",
    "            logger.warning(\"Keine Roh-Artikel zum Verarbeiten gefunden. Breche Pipeline ab.\")\n",
    "            return None\n",
    "\n",
    "        # 2. Daten initial verarbeiten\n",
    "        initially_processed_articles = self._process_articles_initial(raw_articles)\n",
    "        if not initially_processed_articles:\n",
    "            logger.warning(\"Keine Artikel nach initialer LLM-Verarbeitung übrig. Breche Pipeline ab.\")\n",
    "            return None\n",
    "\n",
    "        # 3. NEU: Artikel evaluieren und final auswählen\n",
    "        final_articles_for_newsletter = self._evaluate_and_filter_articles(initially_processed_articles)\n",
    "        if not final_articles_for_newsletter:\n",
    "            logger.warning(\"Keine Artikel nach Evaluierung für den Newsletter ausgewählt. Breche Pipeline ab.\")\n",
    "            return None\n",
    "        \n",
    "        # 4. Newsletter-Datenstruktur erstellen und Sektionen zuordnen\n",
    "        newsletter_sections_map: Dict[str, List[Any]] = {\n",
    "            \"Der Rund um Blick\": [], \"IT & AI\": [], \"Welt und Politik\": [],\n",
    "            \"Wirtschaft\": [], \"Zürich Inside\": [], \"Kultur und Inspiration\": []\n",
    "        }\n",
    "\n",
    "        for article in final_articles_for_newsletter:\n",
    "            target_section = article.category if article.category in newsletter_sections_map else \"Der Rund um Blick\"\n",
    "            newsletter_sections_map[target_section].append(article)\n",
    "\n",
    "        final_sections: List[NewsletterSection] = []\n",
    "        for title, items_list in newsletter_sections_map.items():\n",
    "            if items_list:\n",
    "                if all(isinstance(item, ProcessedArticle) for item in items_list):\n",
    "                    items_list.sort(key=lambda x: (x.relevance_score or 0.0, x.published_at or datetime.min.replace(tzinfo=timezone.utc)), reverse=True)\n",
    "                final_sections.append(NewsletterSection(title=title, items=items_list))\n",
    "        \n",
    "        if not final_sections:\n",
    "            logger.warning(\"Keine Sektionen mit Inhalt für den Newsletter. Breche ab.\")\n",
    "            return None\n",
    "            \n",
    "        newsletter_content = NewsletterData(sections=final_sections)\n",
    "\n",
    "        # 5. Newsletter komponieren (PDF)\n",
    "        logger.info(\"Erstelle Newsletter-PDF...\")\n",
    "        composer = NewsletterComposer(newsletter_content)\n",
    "        pdf_filename_base = f\"Newsletter_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}\"\n",
    "        pdf_filepath = f\"{pdf_filename_base}.pdf\" # Speichert im aktuellen Verzeichnis\n",
    "        \n",
    "        try:\n",
    "            abs_pdf_filepath = composer.generate_pdf(pdf_filepath)\n",
    "            logger.info(f\"PDF '{abs_pdf_filepath}' erfolgreich erstellt.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler beim Erstellen des PDF: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "        # 6. Newsletter verteilen (z.B. Google Drive)\n",
    "        # Hier ist der Code für den Fall, dass du es direkt in Python machst (optional):\n",
    "        # drive_folder_id = get_env_variable(\"GOOGLE_DRIVE_FOLDER_ID\")\n",
    "        # service_account_file = get_env_variable(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "        # if drive_folder_id and service_account_file and os.path.exists(abs_pdf_filepath):\n",
    "        #     logger.info(f\"Lade '{abs_pdf_filepath}' auf Google Drive hoch...\")\n",
    "        #     uploader = GoogleDriveUploader(folder_id=drive_folder_id) # Initialisiert Service automatisch, wenn creds da\n",
    "        #     if uploader.service: # Prüfen, ob Service initialisiert wurde\n",
    "        #         drive_link = uploader.upload_file(local_filepath=abs_pdf_filepath, filename_on_drive=os.path.basename(abs_pdf_filepath))\n",
    "                \n",
    "        #         if drive_link:\n",
    "        #             logger.info(f\"Newsletter erfolgreich auf Google Drive hochgeladen: {drive_link}\")\n",
    "        #             # try:\n",
    "        #             #     os.remove(abs_pdf_filepath) \n",
    "        #             #     logger.info(f\"Lokale PDF-Datei '{abs_pdf_filepath}' gelöscht.\")\n",
    "        #             # except OSError as e_remove:\n",
    "        #             #     logger.warning(f\"Konnte lokale PDF-Datei '{abs_pdf_filepath}' nicht löschen: {e_remove}\")\n",
    "        #             pipeline_duration = datetime.now(timezone.utc) - start_time\n",
    "        #             logger.info(f\"Newsletter-Pipeline in {pipeline_duration} abgeschlossen.\")\n",
    "        #             return drive_link\n",
    "        #         else:\n",
    "        #             logger.error(\"Fehler beim Hochladen des Newsletters auf Google Drive.\")\n",
    "        #     else:\n",
    "        #         logger.warning(\"Google Drive Service nicht initialisiert. Upload übersprungen.\")\n",
    "        # else:\n",
    "        #     logger.warning(f\"Google Drive Upload übersprungen. Bedingungen nicht erfüllt: folder_id='{drive_folder_id}', service_account_file='{service_account_file}', pdf_exists='{os.path.exists(abs_pdf_filepath) if abs_pdf_filepath else False}'.\")\n",
    "        \n",
    "        pipeline_duration = datetime.now(timezone.utc) - start_time\n",
    "        logger.info(f\"Newsletter-Pipeline in {pipeline_duration} abgeschlossen. PDF lokal verfügbar unter: {abs_pdf_filepath}\")\n",
    "        return abs_pdf_filepath\n",
    "\n",
    "# src/utils/config_loader.py\n",
    "# Lädt Umgebungsvariablen und stellt Konfigurationen bereit.\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "from typing import Optional # Optional hinzugefügt\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_env():\n",
    "    \"\"\"Lädt Umgebungsvariablen aus einer .env Datei im Projektwurzelverzeichnis.\"\"\"\n",
    "    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "    dotenv_path = os.path.join(project_root, '.env')\n",
    "    \n",
    "    if os.path.exists(dotenv_path):\n",
    "        load_dotenv(dotenv_path)\n",
    "        logger.debug(f\".env Datei geladen von: {dotenv_path}\")\n",
    "    else:\n",
    "        logger.info(f\".env Datei nicht gefunden unter: {dotenv_path}. Umgebungsvariablen müssen anderweitig gesetzt sein.\")\n",
    "\n",
    "def get_env_variable(variable_name: str, default: Optional[str] = None) -> Optional[str]:\n",
    "    \"\"\"Holt eine Umgebungsvariable. Gibt None zurück oder den Defaultwert, falls nicht gefunden.\"\"\"\n",
    "    value = os.getenv(variable_name, default)\n",
    "    if value is None and default is None: \n",
    "        logger.debug(f\"Umgebungsvariable '{variable_name}' nicht gefunden und kein Defaultwert angegeben.\")\n",
    "    return value\n",
    "\n",
    "def get_api_key(key_name: str) -> str:\n",
    "    \"\"\"Holt einen API-Schlüssel. Löst einen Fehler aus, wenn nicht gefunden.\"\"\"\n",
    "    api_key = os.getenv(key_name)\n",
    "    if not api_key:\n",
    "        logger.critical(f\"Kritischer Fehler: API-Schlüssel '{key_name}' nicht in den Umgebungsvariablen gefunden.\")\n",
    "        raise ValueError(f\"API-Schlüssel '{key_name}' nicht in den Umgebungsvariablen gefunden. Bitte in .env setzen.\")\n",
    "    return api_key\n",
    "\n",
    "# src/utils/logging_setup.py\n",
    "# Konfiguriert das zentrale Logging für die Anwendung.\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from typing import Optional # Optional hinzugefügt\n",
    "\n",
    "def setup_logging(log_level_str: str = \"INFO\", log_file: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Konfiguriert das Logging-System.\n",
    "    log_level_str: Logging-Level als String (z.B. \"DEBUG\", \"INFO\", \"WARNING\").\n",
    "    log_file: Optionaler Pfad zu einer Log-Datei.\n",
    "    \"\"\"\n",
    "    # Hole Default Log-Level aus Umgebungsvariable, falls gesetzt\n",
    "    env_log_level = os.getenv(\"LOG_LEVEL\", log_level_str).upper()\n",
    "    numeric_level = getattr(logging, env_log_level, None)\n",
    "    \n",
    "    if not isinstance(numeric_level, int):\n",
    "        print(f\"Ungültiger Log-Level '{env_log_level}' aus Umgebung oder Default. Verwende INFO.\")\n",
    "        numeric_level = logging.INFO # Fallback auf INFO\n",
    "\n",
    "    # Root-Logger konfigurieren\n",
    "    # Es ist oft besser, Handler nicht immer wieder zu entfernen und hinzuzufügen,\n",
    "    # sondern dies einmalig beim App-Start zu tun.\n",
    "    # Hier stellen wir sicher, dass wir nur Handler hinzufügen, wenn noch keine da sind,\n",
    "    # oder wenn wir spezifische Handler managen wollen.\n",
    "\n",
    "    root_logger = logging.getLogger() # Hole den Root-Logger\n",
    "    \n",
    "    # Setze das Level für den Root-Logger. Alle Handler erben dieses Level, es sei denn, sie haben ein eigenes höheres Level.\n",
    "    root_logger.setLevel(numeric_level) \n",
    "\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "\n",
    "    # Konsolen-Handler hinzufügen, wenn noch keiner für stdout existiert\n",
    "    if not any(isinstance(h, logging.StreamHandler) and h.stream == sys.stdout for h in root_logger.handlers):\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setFormatter(formatter)\n",
    "        # Setze das Level auch für den Handler, falls es feingranularer sein soll als das Root-Level\n",
    "        console_handler.setLevel(numeric_level) \n",
    "        root_logger.addHandler(console_handler)\n",
    "        logging.debug(\"Konsolen-Handler zum Root-Logger hinzugefügt.\")\n",
    "    else:\n",
    "        logging.debug(\"Konsolen-Handler für stdout existiert bereits im Root-Logger.\")\n",
    "\n",
    "\n",
    "    # Hole Log-Datei-Pfad aus Umgebungsvariable, falls gesetzt, überschreibe Parameter\n",
    "    env_log_file = os.getenv(\"LOG_FILE\", log_file)\n",
    "    if env_log_file:\n",
    "        # Datei-Handler hinzufügen, wenn noch keiner für diesen Pfad existiert\n",
    "        if not any(isinstance(h, logging.FileHandler) and h.baseFilename == os.path.abspath(env_log_file) for h in root_logger.handlers):\n",
    "            try:\n",
    "                log_dir = os.path.dirname(env_log_file)\n",
    "                if log_dir and not os.path.exists(log_dir):\n",
    "                    os.makedirs(log_dir)\n",
    "                \n",
    "                file_handler = logging.FileHandler(env_log_file, mode='a', encoding='utf-8')\n",
    "                file_handler.setFormatter(formatter)\n",
    "                file_handler.setLevel(numeric_level) # Setze Level auch für den Handler\n",
    "                root_logger.addHandler(file_handler)\n",
    "                logging.debug(f\"Datei-Handler für '{env_log_file}' zum Root-Logger hinzugefügt.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Fehler beim Erstellen des Datei-Log-Handlers für '{env_log_file}': {e}\", exc_info=True)\n",
    "        else:\n",
    "            logging.debug(f\"Datei-Handler für '{env_log_file}' existiert bereits im Root-Logger.\")\n",
    "\n",
    "\n",
    "    # Test-Log-Nachricht, um zu zeigen, dass das Logging konfiguriert ist\n",
    "    # Diese wird nur ausgegeben, wenn das effektive Level des Loggers INFO oder niedriger ist.\n",
    "    logging.info(f\"Logging konfiguriert mit effektivem Level {logging.getLevelName(root_logger.getEffectiveLevel())}\" + (f\" und potentieller Datei '{env_log_file}'\" if env_log_file else \"\"))\n",
    "\n",
    "\n",
    "# src/models/data_models.py\n",
    "# Pydantic-Modelle zur Definition der Datenstrukturen.\n",
    "\n",
    "from pydantic import BaseModel, HttpUrl, Field, field_validator, model_validator\n",
    "from typing import Optional, List, Dict, Any, Union\n",
    "from datetime import datetime, date, timezone\n",
    "\n",
    "def ensure_timezone_aware(dt_value: Optional[datetime]) -> Optional[datetime]:\n",
    "    \"\"\"Stellt sicher, dass ein datetime-Objekt timezone-aware ist (UTC als Default).\"\"\"\n",
    "    if dt_value is None:\n",
    "        return None\n",
    "    if isinstance(dt_value, str): # Zusätzliche Behandlung, falls String übergeben wird\n",
    "        try:\n",
    "            if dt_value.endswith('Z'):\n",
    "                dt_value = datetime.fromisoformat(dt_value.replace('Z', '+00:00'))\n",
    "            else:\n",
    "                dt_value = datetime.fromisoformat(dt_value)\n",
    "        except ValueError:\n",
    "            # Fallback oder Fehlerbehandlung, wenn String nicht parsebar ist\n",
    "            # Hier geben wir None zurück oder werfen einen Fehler, je nach Anforderung\n",
    "            # Für pydantic ist es oft besser, einen Fehler zu werfen, damit die Validierung fehlschlägt\n",
    "            raise ValueError(f\"Ungültiges Datumsstring-Format für Konvertierung: {dt_value}\")\n",
    "\n",
    "\n",
    "    if dt_value.tzinfo is None or dt_value.tzinfo.utcoffset(dt_value) is None:\n",
    "        return dt_value.replace(tzinfo=timezone.utc)\n",
    "    return dt_value\n",
    "\n",
    "class RawArticle(BaseModel):\n",
    "    title: Optional[str] = None\n",
    "    url: Optional[HttpUrl] = None\n",
    "    description: Optional[str] = None\n",
    "    content_snippet: Optional[str] = None \n",
    "    published_at: Optional[datetime] = None\n",
    "    source_name: Optional[str] = None\n",
    "    source_id: Optional[str] = None\n",
    "\n",
    "    _ensure_published_at_tz_aware = field_validator('published_at', mode='before')(ensure_timezone_aware)\n",
    "\n",
    "\n",
    "class ProcessedArticle(BaseModel):\n",
    "    title: str\n",
    "    url: Optional[HttpUrl] = None\n",
    "    summary: str\n",
    "    category: Optional[str] = \"Unkategorisiert\"\n",
    "    relevance_score: Optional[float] = Field(default=0.0, ge=0.0, le=1.0) \n",
    "    source_name: Optional[str] = None\n",
    "    published_at: Optional[datetime] = None\n",
    "    # llm_processing_details kann jetzt komplexere Infos enthalten\n",
    "    llm_processing_details: Optional[Dict[str, Any]] = Field(default_factory=dict)\n",
    "\n",
    "\n",
    "    _ensure_published_at_tz_aware = field_validator('published_at', mode='before')(ensure_timezone_aware)\n",
    "\n",
    "    # Beispiel für model_validator, falls benötigt\n",
    "    # @model_validator(mode='after')\n",
    "    # def check_consistency(self) -> 'ProcessedArticle':\n",
    "    #     if self.relevance_score is not None and (self.relevance_score < 0 or self.relevance_score > 1):\n",
    "    #         raise ValueError(\"Relevance score must be between 0 and 1\")\n",
    "    #     return self\n",
    "\n",
    "\n",
    "class Event(BaseModel):\n",
    "    summary: str \n",
    "    start_time: Optional[datetime] = None\n",
    "    end_time: Optional[datetime] = None\n",
    "    location: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "    url: Optional[HttpUrl] = None\n",
    "    source: str \n",
    "\n",
    "    _ensure_start_time_tz_aware = field_validator('start_time', mode='before')(ensure_timezone_aware)\n",
    "    _ensure_end_time_tz_aware = field_validator('end_time', mode='before')(ensure_timezone_aware)\n",
    "\n",
    "\n",
    "class Birthday(BaseModel):\n",
    "    name: str\n",
    "    date_month: int = Field(ge=1, le=12)\n",
    "    date_day: int = Field(ge=1, le=31)\n",
    "    original_date_info: Optional[str] = None \n",
    "    source: str \n",
    "\n",
    "\n",
    "class WeatherInfo(BaseModel):\n",
    "    location: str\n",
    "    temperature_celsius: float\n",
    "    condition: str \n",
    "    humidity_percent: Optional[float] = None\n",
    "    wind_speed_kmh: Optional[float] = None\n",
    "    icon_url: Optional[HttpUrl] = None \n",
    "    forecast_snippet: Optional[str] = None \n",
    "\n",
    "\n",
    "class NewsletterSection(BaseModel):\n",
    "    title: str \n",
    "    items: List[Union[ProcessedArticle, Event, Birthday, WeatherInfo, Dict[str, Any]]]\n",
    "\n",
    "\n",
    "class NewsletterData(BaseModel):\n",
    "    generation_date: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n",
    "    title: str = \"Dein personalisierter Newsletter\"\n",
    "    sections: List[NewsletterSection]\n",
    "\n",
    "    _ensure_generation_date_tz_aware = field_validator('generation_date', mode='before')(ensure_timezone_aware)\n",
    "\n",
    "# src/agents/data_fetchers/base_fetcher.py\n",
    "# Abstrakte Basisklasse für alle Datenbeschaffer-Agenten.\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__) # Logger für dieses Modul\n",
    "\n",
    "class BaseDataFetcher(ABC):\n",
    "    \"\"\"\n",
    "    Abstrakte Basisklasse für Agenten, die Daten von externen Quellen abrufen.\n",
    "    \"\"\"\n",
    "    def __init__(self, source_name: str):\n",
    "        \"\"\"\n",
    "        Initialisiert den Fetcher mit einem Namen für die Quelle.\n",
    "        Args:\n",
    "            source_name (str): Ein identifizierbarer Name für die Datenquelle (z.B. \"NewsAPI\", \"Tagesschau RSS\").\n",
    "        \"\"\"\n",
    "        self.source_name = source_name\n",
    "        logger.info(f\"Initialisiere Datenbeschaffer für Quelle: '{self.source_name}'.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def fetch_data(self) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Abstrakte Methode zum Abrufen von Daten.\n",
    "        Muss von abgeleiteten Klassen implementiert werden.\n",
    "\n",
    "        Returns:\n",
    "            List[Any]: Eine Liste von abgerufenen Datenelementen (z.B. RawArticle, Event).\n",
    "                       Die genaue Struktur hängt vom spezifischen Fetcher ab.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"<{self.__class__.__name__}(source_name='{self.source_name}')>\"\n",
    "\n",
    "# src/agents/data_fetchers/newsapi_fetcher.py\n",
    "# Datenbeschaffer für Nachrichten von NewsAPI.org.\n",
    "\n",
    "import requests\n",
    "from typing import List, Optional\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from src.models.data_models import RawArticle\n",
    "from src.utils.config_loader import get_api_key\n",
    "from .base_fetcher import BaseDataFetcher\n",
    "import logging\n",
    "import json # Für das Parsen von Fehlermeldungen\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NewsAPIFetcher(BaseDataFetcher):\n",
    "    \"\"\"\n",
    "    Ruft Nachrichtenartikel vom NewsAPI.org Dienst ab.\n",
    "    Kann entweder den /v2/everything oder /v2/top-headlines Endpunkt verwenden.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 query: Optional[str] = None, \n",
    "                 country: Optional[str] = None,\n",
    "                 category: Optional[str] = None,\n",
    "                 sources: Optional[str] = None, # Komma-separierte Quellen-IDs\n",
    "                 language: str = \"de\", \n",
    "                 days_ago: int = 1, \n",
    "                 endpoint: str = \"everything\", \n",
    "                 page_size: int = 20,\n",
    "                 source_name_override: Optional[str] = None):\n",
    "        \n",
    "        effective_source_name = source_name_override if source_name_override else f\"NewsAPI ({endpoint})\"\n",
    "        super().__init__(source_name=effective_source_name)\n",
    "        \n",
    "        self.api_key = get_api_key(\"NEWSAPI_API_KEY\")\n",
    "        self.base_url = \"https://newsapi.org/v2/\"\n",
    "        self.query = query\n",
    "        self.country = country\n",
    "        self.category = category\n",
    "        self.sources = sources\n",
    "        self.language = language\n",
    "        self.days_ago = days_ago\n",
    "        self.endpoint = endpoint.lower()\n",
    "        self.page_size = page_size\n",
    "\n",
    "        if self.endpoint not in [\"everything\", \"top-headlines\"]:\n",
    "            logger.error(f\"Ungültiger NewsAPI Endpunkt '{self.endpoint}' für Quelle '{self.source_name}'.\")\n",
    "            raise ValueError(\"Ungültiger Endpunkt für NewsAPI. Muss 'everything' oder 'top-headlines' sein.\")\n",
    "        \n",
    "        if self.endpoint == \"top-headlines\" and self.sources and (self.country or self.category):\n",
    "            logger.warning(f\"NewsAPI: Für /top-headlines wird 'sources' verwendet, 'country'/'category' werden ignoriert für Quelle '{self.source_name}'.\")\n",
    "\n",
    "\n",
    "    def _get_from_date(self) -> str:\n",
    "        \"\"\"Erstellt den Datumsstring für den 'from'-Parameter des 'everything'-Endpunkts.\"\"\"\n",
    "        # Nutze UTC für Konsistenz bei Datumsberechnungen\n",
    "        return (datetime.now(timezone.utc) - timedelta(days=self.days_ago)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    def fetch_data(self) -> List[RawArticle]:\n",
    "        params = {\n",
    "            \"apiKey\": self.api_key,\n",
    "            \"language\": self.language,\n",
    "            \"pageSize\": self.page_size,\n",
    "        }\n",
    "\n",
    "        if self.endpoint == \"everything\":\n",
    "            if not self.query:\n",
    "                logger.warning(f\"Für den 'everything'-Endpunkt von '{self.source_name}' wird ein Suchbegriff ('query') dringend empfohlen.\")\n",
    "            params[\"q\"] = self.query if self.query else \"Aktuelles\" \n",
    "            params[\"from\"] = self._get_from_date()\n",
    "            params[\"sortBy\"] = \"publishedAt\"\n",
    "        \n",
    "        elif self.endpoint == \"top-headlines\":\n",
    "            # Priorisierung: sources > country/category > q (für Top-Headlines)\n",
    "            if self.sources:\n",
    "                params[\"sources\"] = self.sources\n",
    "            elif self.country:\n",
    "                params[\"country\"] = self.country\n",
    "                if self.category: \n",
    "                    params[\"category\"] = self.category\n",
    "            elif self.category:\n",
    "                params[\"category\"] = self.category\n",
    "            if self.query: # 'q' kann auch mit /top-headlines verwendet werden\n",
    "                params[\"q\"] = self.query\n",
    "            if not (self.sources or self.country or self.category or self.query):\n",
    "                logger.error(f\"Für NewsAPI /top-headlines muss mindestens einer der Parameter 'sources', 'country', 'category' oder 'q' gesetzt sein für Quelle '{self.source_name}'.\")\n",
    "                return [] # Leere Liste, da die Anfrage so nicht sinnvoll ist\n",
    "        \n",
    "        url = f\"{self.base_url}{self.endpoint}\"\n",
    "        params_to_log = {k: v for k, v in params.items() if k != 'apiKey'}\n",
    "        logger.info(f\"Frage NewsAPI ({self.source_name}) ab: {url} mit Parametern: {params_to_log}\")\n",
    "        \n",
    "        raw_articles: List[RawArticle] = []\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=20)\n",
    "            \n",
    "            # Logge Status und einen Teil der Antwort immer, um Debugging zu erleichtern\n",
    "            logger.debug(f\"NewsAPI ({self.source_name}) Status Code: {response.status_code}\")\n",
    "            # Logge nur einen Teil der Antwort, um Logs nicht zu überfluten\n",
    "            # response_snippet = response.text[:500] + \"...\" if response.text else \"Keine Antwort\"\n",
    "            # logger.debug(f\"NewsAPI ({self.source_name}) Antwort Snippet: {response_snippet}\")\n",
    "\n",
    "            response.raise_for_status() \n",
    "            data = response.json()\n",
    "\n",
    "            if data.get(\"status\") == \"error\":\n",
    "                logger.error(f\"NewsAPI Fehler ({self.source_name}): Code '{data.get('code')}', Message '{data.get('message')}'\")\n",
    "                return []\n",
    "\n",
    "            articles_data = data.get(\"articles\", [])\n",
    "            logger.info(f\"NewsAPI ({self.source_name}) lieferte {data.get('totalResults', 0)} Gesamtartikel, {len(articles_data)} im aktuellen Batch.\")\n",
    "\n",
    "            for article_data in articles_data:\n",
    "                try:\n",
    "                    # publishedAt Parsing verbessert\n",
    "                    published_at_str = article_data.get(\"publishedAt\")\n",
    "                    published_dt: Optional[datetime] = None\n",
    "                    if published_at_str:\n",
    "                        try:\n",
    "                            # Stellt sicher, dass das Datum als timezone-aware UTC geparsed wird\n",
    "                            # datetime.fromisoformat kann 'Z' direkt parsen seit Python 3.11\n",
    "                            # Für ältere Versionen ist .replace('Z', '+00:00') nötig.\n",
    "                            if published_at_str.endswith('Z'):\n",
    "                                 published_dt = datetime.fromisoformat(published_at_str[:-1] + '+00:00')\n",
    "                            else: # Versuche, es direkt zu parsen\n",
    "                                temp_dt = datetime.fromisoformat(published_at_str)\n",
    "                                # Mache es timezone-aware, falls es das nicht ist (nehme UTC an)\n",
    "                                if temp_dt.tzinfo is None or temp_dt.tzinfo.utcoffset(temp_dt) is None:\n",
    "                                    published_dt = temp_dt.replace(tzinfo=timezone.utc)\n",
    "                                else:\n",
    "                                    published_dt = temp_dt # Bereits timezone-aware\n",
    "                        except ValueError:\n",
    "                            logger.warning(f\"Konnte publishedAt '{published_at_str}' nicht parsen für Artikel von '{self.source_name}': {article_data.get('title')}\")\n",
    "                    \n",
    "                    raw_articles.append(\n",
    "                        RawArticle(\n",
    "                            title=article_data.get(\"title\"),\n",
    "                            url=str(article_data.get(\"url\")) if article_data.get(\"url\") else None, # Stelle sicher, dass es ein String ist für Pydantic HttpUrl\n",
    "                            description=article_data.get(\"description\"),\n",
    "                            content_snippet=article_data.get(\"content\"),\n",
    "                            published_at=published_dt,\n",
    "                            source_name=article_data.get(\"source\", {}).get(\"name\", self.source_name),\n",
    "                            source_id=article_data.get(\"source\", {}).get(\"id\")\n",
    "                        )\n",
    "                    )\n",
    "                except Exception as e_article_parse:\n",
    "                    logger.warning(f\"Überspringe Artikel von '{self.source_name}' aufgrund eines Parsing-Fehlers: '{article_data.get('title', 'Unbekannt')}' - Fehler: {e_article_parse}\", exc_info=False)\n",
    "            \n",
    "            logger.info(f\"{len(raw_articles)} Artikel erfolgreich von '{self.source_name}' abgerufen und geparsed.\")\n",
    "\n",
    "        except requests.exceptions.HTTPError as e_http:\n",
    "            error_message = f\"HTTP-Fehler ({e_http.response.status_code}) beim Abrufen von '{self.source_name}'\"\n",
    "            try:\n",
    "                error_details = e_http.response.json()\n",
    "                error_message += f\": {error_details.get('code')} - {error_details.get('message')}\"\n",
    "            except json.JSONDecodeError:\n",
    "                error_message += f\". Rohantwort: {e_http.response.text[:200]}\" # Snippet der Rohantwort\n",
    "            logger.error(error_message, exc_info=False) # exc_info=False um Log nicht mit vollem Traceback zu überfluten\n",
    "        except requests.exceptions.RequestException as e_req:\n",
    "            logger.error(f\"Netzwerkfehler beim Abrufen von Daten von '{self.source_name}': {e_req}\", exc_info=True)\n",
    "        except Exception as e_general:\n",
    "            logger.error(f\"Unerwarteter Fehler beim Verarbeiten von Daten von '{self.source_name}': {e_general}\", exc_info=True)\n",
    "        \n",
    "        return raw_articles\n",
    "\n",
    "# src/agents/llm_processors/base_processor.py\n",
    "# Abstrakte Basisklasse für alle LLM-basierten Verarbeitungs-Agenten.\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Optional\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from src.utils.config_loader import get_api_key, get_env_variable\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BaseLLMProcessor(ABC):\n",
    "    \"\"\"\n",
    "    Abstrakte Basisklasse für Agenten, die Daten mithilfe eines LLM verarbeiten.\n",
    "    Verwendet standardmäßig Google Gemini.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 model_name: Optional[str] = None, \n",
    "                 temperature: float = 0.3,\n",
    "                 llm_provider: str = \"gemini\"): \n",
    "        self.llm_provider = llm_provider.lower()\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.llm: Optional[BaseChatModel] = None\n",
    "\n",
    "        try:\n",
    "            if self.llm_provider == \"gemini\":\n",
    "                gemini_api_key = get_api_key(\"GOOGLE_API_KEY\") \n",
    "                # Hole Default-Modellnamen aus .env, falls vorhanden, sonst Fallback\n",
    "                default_gemini_model = get_env_variable(\"GEMINI_DEFAULT_MODEL\", \"gemini-1.5-flash-latest\")\n",
    "                self.model_name = model_name if model_name else default_gemini_model\n",
    "                \n",
    "                self.llm = ChatGoogleGenerativeAI(\n",
    "                    model=self.model_name,\n",
    "                    google_api_key=gemini_api_key,\n",
    "                    temperature=self.temperature,\n",
    "                    convert_system_message_to_human=True \n",
    "                )\n",
    "                logger.info(f\"Initialisiere Gemini LLM Processor mit Modell: {self.model_name}, Temperatur: {self.temperature}.\")\n",
    "            else:\n",
    "                logger.error(f\"Nicht unterstützter LLM-Provider: {self.llm_provider}\")\n",
    "                raise ValueError(f\"Nicht unterstützter LLM-Provider: {self.llm_provider}\")\n",
    "        except Exception as e:\n",
    "            logger.critical(f\"Fehler bei der Initialisierung des LLM-Clients für Provider '{self.llm_provider}': {e}\", exc_info=True)\n",
    "            raise \n",
    "\n",
    "    @abstractmethod\n",
    "    def process(self, data: Any, **kwargs) -> Any:\n",
    "        if self.llm is None:\n",
    "            logger.error(\"LLM wurde nicht korrekt initialisiert. Verarbeitung nicht möglich.\")\n",
    "            raise RuntimeError(\"LLM nicht initialisiert. Verarbeitung abgebrochen.\")\n",
    "        pass\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"<{self.__class__.__name__}(model='{self.model_name}', provider='{self.llm_provider}')>\"\n",
    "\n",
    "# src/agents/llm_processors/summarizer_agent.py\n",
    "# LLM-Agent zum Zusammenfassen von Texten (z.B. Artikel).\n",
    "\n",
    "from typing import List, Union, Optional # Optional hinzugefügt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from src.models.data_models import RawArticle, ProcessedArticle, Event \n",
    "from .base_processor import BaseLLMProcessor\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SummarizerAgent(BaseLLMProcessor):\n",
    "    def __init__(self, \n",
    "                 model_name: Optional[str] = None, \n",
    "                 temperature: float = 0.2): \n",
    "        super().__init__(model_name=model_name, temperature=temperature)\n",
    "        \n",
    "        self.prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"human\", \n",
    "             \"\"\"\n",
    "Du bist ein Experte im Verfassen prägnanter Nachrichten-Zusammenfassungen für einen anspruchsvollen Leser.\n",
    "Bitte fasse den folgenden Text für einen Newsletter zusammen. Die Zusammenfassung sollte die Kernbotschaft in 2-4 prägnanten Sätzen wiedergeben.\n",
    "Konzentriere dich auf die wichtigsten Fakten und Implikationen. Vermeide Füllwörter.\n",
    "Gib NUR die reine Zusammenfassung zurück, ohne zusätzliche Einleitungen, Höflichkeitsfloskeln oder Kommentare.\n",
    "\n",
    "ARTIKELTITEL: {title}\n",
    "ARTIKELBESCHREIBUNG (falls vorhanden): {description}\n",
    "ARTIKELINHALT (Auszug, falls vorhanden, sonst leer): {content_snippet}\n",
    "\n",
    "ZUSAMMENFASSUNG:\"\"\")\n",
    "        ])\n",
    "        \n",
    "        self.chain = self.prompt_template | self.llm | StrOutputParser()\n",
    "        logger.info(\"SummarizerAgent Kette initialisiert.\")\n",
    "\n",
    "    def _get_text_for_summarization(self, item: Union[RawArticle, Event]) -> str:\n",
    "        text_candidates = []\n",
    "        if isinstance(item, RawArticle):\n",
    "            if item.content_snippet and len(item.content_snippet.strip()) > 50: # Bevorzuge längeren Inhalt\n",
    "                text_candidates.append(item.content_snippet.strip())\n",
    "            if item.description and len(item.description.strip()) > 20:\n",
    "                text_candidates.append(item.description.strip())\n",
    "            if item.title: # Titel als letzter Fallback, falls alles andere fehlt\n",
    "                text_candidates.append(item.title.strip())\n",
    "        elif isinstance(item, Event):\n",
    "            if item.description and len(item.description.strip()) > 20:\n",
    "                text_candidates.append(item.description.strip())\n",
    "            if item.summary: # summary ist Titel des Events\n",
    "                 text_candidates.append(item.summary.strip())\n",
    "        \n",
    "        # Wähle den längsten, sinnvollen Kandidaten\n",
    "        if text_candidates:\n",
    "            return max(text_candidates, key=len)\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "    def summarize_article(self, article: RawArticle) -> str:\n",
    "        logger.debug(f\"Zusammenfassung für Artikel angefordert: '{article.title or 'Unbekannter Titel'}'\")\n",
    "        \n",
    "        text_to_summarize = self._get_text_for_summarization(article)\n",
    "        \n",
    "        if not text_to_summarize:\n",
    "            logger.warning(f\"Kein ausreichender Inhalt zum Zusammenfassen für Artikel: {article.title or 'Unbekannter Titel'}\")\n",
    "            return article.description or article.title or \"Keine Zusammenfassung verfügbar (unzureichender Inhalt).\"\n",
    "\n",
    "        try:\n",
    "            max_input_chars = 4000 # Etwas erhöht, aber immer noch vorsichtig\n",
    "            \n",
    "            summary_result = self.chain.invoke({\n",
    "                \"title\": article.title or \"Kein Titel\",\n",
    "                \"description\": article.description or \"\", # description kann auch leer sein\n",
    "                \"content_snippet\": text_to_summarize[:max_input_chars] \n",
    "            })\n",
    "            \n",
    "            logger.debug(f\"Zusammenfassung für '{article.title}' erstellt.\")\n",
    "            return summary_result.strip() if summary_result else \"Zusammenfassung konnte nicht erstellt werden.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler beim Zusammenfassen des Artikels '{article.title}': {e}\", exc_info=True)\n",
    "            return article.description or article.title or \"Zusammenfassung fehlgeschlagen (LLM-Fehler).\"\n",
    "\n",
    "    def process(self, articles: List[RawArticle]) -> List[ProcessedArticle]:\n",
    "        if self.llm is None: \n",
    "             logger.error(\"LLM nicht initialisiert im SummarizerAgent. Breche Verarbeitung ab.\")\n",
    "             return []\n",
    "             \n",
    "        logger.info(f\"Starte Batch-Zusammenfassung für {len(articles)} Artikel.\")\n",
    "        processed_articles_list: List[ProcessedArticle] = []\n",
    "        \n",
    "        for article_item in articles:\n",
    "            summary_text = self.summarize_article(article_item)\n",
    "            processed_articles_list.append(\n",
    "                ProcessedArticle(\n",
    "                    title=article_item.title or \"Unbekannter Titel\",\n",
    "                    url=article_item.url,\n",
    "                    summary=summary_text,\n",
    "                    source_name=article_item.source_name,\n",
    "                    published_at=article_item.published_at\n",
    "                )\n",
    "            )\n",
    "        logger.info(f\"Batch-Zusammenfassung für {len(processed_articles_list)} Artikel abgeschlossen.\")\n",
    "        return processed_articles_list\n",
    "\n",
    "# src/agents/llm_processors/categorizer_agent.py\n",
    "# LLM-Agent zum Kategorisieren von Texten (z.B. Artikel).\n",
    "\n",
    "from typing import List, Optional, Union, Dict # Union, Dict hinzugefügt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser # Behalte JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel as LangchainBaseModel, Field as LangchainField # Field hinzugefügt\n",
    "from src.models.data_models import RawArticle \n",
    "from .base_processor import BaseLLMProcessor\n",
    "import logging\n",
    "import json \n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CategorizationResponse(LangchainBaseModel):\n",
    "    category: str = LangchainField(description=\"Die am besten passende Kategorie für den Artikel aus der vorgegebenen Liste.\")\n",
    "    # Optional: confidence und reasoning können beibehalten oder entfernt werden, je nach Bedarf.\n",
    "    # confidence: Optional[float] = LangchainField(description=\"Konfidenzwert der Kategorisierung (0.0 bis 1.0).\", default=None)\n",
    "    # reasoning: Optional[str] = LangchainField(description=\"Kurze Begründung für die gewählte Kategorie.\", default=None)\n",
    "\n",
    "\n",
    "class CategorizerAgent(BaseLLMProcessor):\n",
    "    def __init__(self, \n",
    "                 categories: List[str],\n",
    "                 model_name: Optional[str] = None,\n",
    "                 temperature: float = 0.1): \n",
    "        super().__init__(model_name=model_name, temperature=temperature)\n",
    "        \n",
    "        if not categories:\n",
    "            logger.error(\"CategorizerAgent erfordert eine Liste von Kategorien bei der Initialisierung.\")\n",
    "            raise ValueError(\"Es müssen Kategorien für den CategorizerAgent bereitgestellt werden.\")\n",
    "        self.categories_list = categories # Speichere als Liste für die Validierung\n",
    "        self.categories_str = \", \".join(f\"'{cat}'\" for cat in categories)\n",
    "\n",
    "        self.prompt_template = ChatPromptTemplate.from_template( # Vereinfacht zu from_template\n",
    "             \"\"\"\n",
    "Du bist ein Experte für die thematische Kategorisierung von Nachrichtenartikeln.\n",
    "Deine Aufgabe ist es, den folgenden Artikel EINER der vorgegebenen Kategorien zuzuordnen.\n",
    "Antworte ausschließlich im JSON-Format. Das JSON-Objekt muss einen Schlüssel \"category\" enthalten, dessen Wert eine der unten genannten Kategorien ist.\n",
    "\n",
    "Vorgegebene Kategorien: [{available_categories}]\n",
    "\n",
    "Wähle nur eine einzige, die relevanteste Kategorie.\n",
    "\n",
    "ARTIKELTITEL: {title}\n",
    "ARTIKELZUSAMMENFASSUNG oder TEXTAUSZUG: {text_snippet}\n",
    "\n",
    "JSON-ANTWORT (nur das JSON-Objekt, ohne Markdown):\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.output_parser = JsonOutputParser(pydantic_object=CategorizationResponse)\n",
    "        self.chain = self.prompt_template | self.llm | self.output_parser\n",
    "        logger.info(f\"CategorizerAgent Kette initialisiert mit Kategorien: {self.categories_str}.\")\n",
    "\n",
    "    def _get_text_for_categorization(self, item: RawArticle, summary: Optional[str] = None) -> str:\n",
    "        text_candidates = []\n",
    "        if summary and len(summary.strip()) > 20: # Bevorzuge Zusammenfassung, wenn vorhanden\n",
    "            text_candidates.append(summary.strip())\n",
    "        \n",
    "        if isinstance(item, RawArticle):\n",
    "            # Füge Beschreibung hinzu, wenn keine gute Zusammenfassung da ist oder sie kurz ist\n",
    "            if item.description and len(item.description.strip()) > 20:\n",
    "                text_candidates.append(item.description.strip())\n",
    "            # Titel als letzter Ausweg oder zur Ergänzung\n",
    "            if item.title:\n",
    "                text_candidates.append(item.title.strip())\n",
    "        \n",
    "        if text_candidates:\n",
    "            # Kombiniere Titel und längsten anderen Text für mehr Kontext, aber halte es kurz\n",
    "            title_part = item.title if item.title else \"\"\n",
    "            best_other_text = max((tc for tc in text_candidates if tc != title_part), key=len, default=\"\")\n",
    "            # return f\"{title_part} - {best_other_text}\".strip(\" -\") if best_other_text else title_part\n",
    "            # Für reine Kategorisierung ist der längste verfügbare Text oft am besten:\n",
    "            return max(text_candidates, key=len)\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "    def categorize_article(self, article: RawArticle, summary_for_categorization: Optional[str] = None) -> str:\n",
    "        logger.debug(f\"Kategorisierung für Artikel angefordert: '{article.title or 'Unbekannter Titel'}'\")\n",
    "        \n",
    "        text_to_categorize = self._get_text_for_categorization(article, summary_for_categorization)\n",
    "        \n",
    "        if not text_to_categorize:\n",
    "            logger.warning(f\"Kein ausreichender Inhalt zum Kategorisieren für Artikel: {article.title or 'Unbekannter Titel'}\")\n",
    "            return \"Unkategorisiert\" \n",
    "\n",
    "        try:\n",
    "            max_input_chars = 2000 \n",
    "            \n",
    "            # Das Ergebnis des JsonOutputParser ist bereits ein Dictionary (oder das Pydantic-Objekt)\n",
    "            response_dict: Dict = self.chain.invoke({\n",
    "                \"available_categories\": self.categories_str,\n",
    "                \"title\": article.title or \"Kein Titel\",\n",
    "                \"text_snippet\": text_to_categorize[:max_input_chars] \n",
    "            })\n",
    "            \n",
    "            category = response_dict.get(\"category\", \"Unkategorisiert\")\n",
    "\n",
    "            # Stelle sicher, dass die zurückgegebene Kategorie eine der erlaubten ist\n",
    "            if category not in self.categories_list and category != \"Unkategorisiert\":\n",
    "                logger.warning(f\"LLM gab eine ungültige Kategorie '{category}' zurück für Artikel '{article.title}'. Setze auf 'Unkategorisiert'.\")\n",
    "                category = \"Unkategorisiert\"\n",
    "\n",
    "            logger.debug(f\"Artikel '{article.title}' kategorisiert als: '{category}'.\")\n",
    "            return category\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler beim Kategorisieren des Artikels '{article.title}': {e}\", exc_info=True)\n",
    "            return \"Fehler bei Kategorisierung\" \n",
    "\n",
    "    def process(self, articles_with_summaries: List[Dict[str, Union[RawArticle, str]]]) -> List[Dict[str, Union[RawArticle, str, str]]]:\n",
    "        if self.llm is None:\n",
    "             logger.error(\"LLM nicht initialisiert im CategorizerAgent. Breche Verarbeitung ab.\")\n",
    "             return []\n",
    "             \n",
    "        logger.info(f\"Starte Batch-Kategorisierung für {len(articles_with_summaries)} Artikel.\")\n",
    "        categorized_items_list = []\n",
    "        \n",
    "        for item_data_dict in articles_with_summaries:\n",
    "            raw_article_obj = item_data_dict.get('raw_article')\n",
    "            summary_text = item_data_dict.get('summary')\n",
    "            if not isinstance(raw_article_obj, RawArticle):\n",
    "                logger.warning(f\"Ungültiges Item für Kategorisierung übersprungen: {raw_article_obj}\")\n",
    "                continue\n",
    "\n",
    "            category_name = self.categorize_article(raw_article_obj, summary_text)\n",
    "            item_data_dict['category'] = category_name # Füge Kategorie zum existierenden Dict hinzu\n",
    "            categorized_items_list.append(item_data_dict)\n",
    "            \n",
    "        logger.info(f\"Batch-Kategorisierung für {len(categorized_items_list)} Artikel abgeschlossen.\")\n",
    "        return categorized_items_list\n",
    "\n",
    "# src/agents/llm_processors/evaluator_agent.py\n",
    "# NEUE DATEI: LLM-Agent zur Evaluierung von Artikeln (z.B. Zielgruppenrelevanz)\n",
    "\n",
    "from typing import List, Dict, Optional, Union\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel as LangchainBaseModel, Field as LangchainField\n",
    "from src.models.data_models import ProcessedArticle # Arbeitet mit bereits zusammengefassten Artikeln\n",
    "from .base_processor import BaseLLMProcessor\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Pydantic-Modell für die erwartete JSON-Ausgabe der Evaluierung\n",
    "class ArticleEvaluationResponse(LangchainBaseModel):\n",
    "    relevance_score: float = LangchainField(description=\"Ein Score von 0.0 (irrelevant) bis 1.0 (hoch relevant) für die Zielgruppe.\", ge=0.0, le=1.0)\n",
    "    is_recommended: bool = LangchainField(description=\"True, wenn der Artikel für den Newsletter empfohlen wird, sonst False.\")\n",
    "    feedback_summary: Optional[str] = LangchainField(description=\"Kurzes Feedback oder Begründung für die Bewertung.\", default=None)\n",
    "    identified_keywords: Optional[List[str]] = LangchainField(description=\"Schlüsselwörter im Artikel, die zur Relevanz beitragen.\", default_factory=list)\n",
    "\n",
    "class AudienceAlignmentAgent(BaseLLMProcessor):\n",
    "    \"\"\"\n",
    "    Ein LLM-Agent, der bewertet, wie gut ein Artikel zur Zielgruppe des Newsletters passt.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 user_profile: str, # Beschreibung der Zielgruppe/Nutzerinteressen\n",
    "                 model_name: Optional[str] = None,\n",
    "                 temperature: float = 0.2):\n",
    "        super().__init__(model_name=model_name, temperature=temperature)\n",
    "        \n",
    "        if not user_profile or len(user_profile.strip()) < 20: # Mindestlänge für ein sinnvolles Profil\n",
    "            logger.error(\"AudienceAlignmentAgent erfordert ein aussagekräftiges Nutzerprofil.\")\n",
    "            raise ValueError(\"Ein detailliertes Nutzerprofil ist für den AudienceAlignmentAgent erforderlich.\")\n",
    "        self.user_profile = user_profile\n",
    "\n",
    "        self.prompt_template = ChatPromptTemplate.from_template(\n",
    "             \"\"\"\n",
    "Du bist ein kritischer Redakteur, der die Relevanz von Nachrichtenartikeln für eine spezifische Zielgruppe bewertet.\n",
    "Deine Aufgabe ist es, den folgenden Artikel im Kontext des gegebenen Nutzerprofils zu analysieren.\n",
    "Antworte ausschließlich im JSON-Format gemäß der folgenden Struktur:\n",
    "{{\n",
    "  \"relevance_score\": float (0.0 bis 1.0, wobei 1.0 perfekt passt),\n",
    "  \"is_recommended\": boolean (true, wenn der Artikel für den Newsletter basierend auf dem Profil empfohlen wird),\n",
    "  \"feedback_summary\": string (kurze Begründung für deine Bewertung und Empfehlung, max. 2 Sätze),\n",
    "  \"identified_keywords\": list[string] (3-5 Schlüsselwörter aus dem Artikel, die für die Relevanzentscheidung wichtig waren)\n",
    "}}\n",
    "\n",
    "NUTZERPROFIL / ZIELGRUPPE:\n",
    "---\n",
    "{user_profile_description}\n",
    "---\n",
    "\n",
    "ZU BEWERTENDER ARTIKEL:\n",
    "Titel: {article_title}\n",
    "Zusammenfassung: {article_summary}\n",
    "Quelle: {article_source}\n",
    "Kategorie (falls bekannt): {article_category}\n",
    "\n",
    "JSON-BEWERTUNG:\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.output_parser = JsonOutputParser(pydantic_object=ArticleEvaluationResponse)\n",
    "        self.chain = self.prompt_template | self.llm | self.output_parser\n",
    "        logger.info(f\"AudienceAlignmentAgent Kette initialisiert.\")\n",
    "\n",
    "    def evaluate_article(self, article: ProcessedArticle) -> Dict:\n",
    "        \"\"\"Bewertet einen einzelnen ProcessedArticle auf Zielgruppenrelevanz.\"\"\"\n",
    "        logger.debug(f\"Bewerte Artikel auf Zielgruppenrelevanz: '{article.title}'\")\n",
    "        \n",
    "        if not article.summary or len(article.summary.strip()) < 10:\n",
    "            logger.warning(f\"Artikel '{article.title}' hat keine ausreichende Zusammenfassung für die Bewertung. Gebe niedrigen Score.\")\n",
    "            return {\"relevance_score\": 0.1, \"is_recommended\": False, \"feedback_summary\": \"Keine ausreichende Zusammenfassung für eine Bewertung.\", \"identified_keywords\": []}\n",
    "\n",
    "        try:\n",
    "            max_summary_chars = 1000 # Nur die Zusammenfassung verwenden\n",
    "            \n",
    "            response_data: Union[Dict, ArticleEvaluationResponse] = self.chain.invoke({\n",
    "                \"user_profile_description\": self.user_profile,\n",
    "                \"article_title\": article.title,\n",
    "                \"article_summary\": article.summary[:max_summary_chars],\n",
    "                \"article_source\": article.source_name or \"Unbekannt\",\n",
    "                \"article_category\": article.category or \"Unkategorisiert\"\n",
    "            })\n",
    "            \n",
    "            # Konvertiere Pydantic-Modell zu Dict, falls es nicht schon eines ist\n",
    "            if isinstance(response_data, LangchainBaseModel):\n",
    "                evaluation_dict = response_data.dict()\n",
    "            else:\n",
    "                evaluation_dict = response_data # Bereits ein Dict\n",
    "\n",
    "            logger.debug(f\"Evaluierung für '{article.title}': Score={evaluation_dict.get('relevance_score')}, Empfohlen={evaluation_dict.get('is_recommended')}\")\n",
    "            return evaluation_dict\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler bei der Evaluierung des Artikels '{article.title}': {e}\", exc_info=True)\n",
    "            # Fallback-Bewertung bei Fehler\n",
    "            return {\"relevance_score\": 0.0, \"is_recommended\": False, \"feedback_summary\": f\"Fehler bei der Evaluierung: {str(e)[:100]}\", \"identified_keywords\": []}\n",
    "\n",
    "    def process(self, articles: List[ProcessedArticle]) -> List[Dict]: # Gibt eine Liste von Evaluations-Dicts zurück\n",
    "        \"\"\"Verarbeitet eine Liste von ProcessedArticle-Objekten und gibt für jeden eine Bewertung zurück.\"\"\"\n",
    "        if self.llm is None:\n",
    "             logger.error(\"LLM nicht initialisiert im AudienceAlignmentAgent. Breche Verarbeitung ab.\")\n",
    "             return []\n",
    "             \n",
    "        logger.info(f\"Starte Batch-Evaluierung für {len(articles)} Artikel.\")\n",
    "        evaluations = []\n",
    "        for article_item in articles:\n",
    "            evaluation = self.evaluate_article(article_item)\n",
    "            evaluations.append(evaluation) # Füge das gesamte Evaluations-Dict hinzu\n",
    "            \n",
    "        logger.info(f\"Batch-Evaluierung für {len(evaluations)} Artikel abgeschlossen.\")\n",
    "        return evaluations\n",
    "\n",
    "# src/agents/newsletter_composer.py\n",
    "# Erstellt den Newsletter (z.B. als PDF).\n",
    "\n",
    "from typing import List, Any, Dict\n",
    "from src.models.data_models import NewsletterData, NewsletterSection, ProcessedArticle, Event, Birthday, WeatherInfo\n",
    "from datetime import datetime, timezone\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak, KeepInFrame\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch, cm\n",
    "from reportlab.lib.enums import TA_JUSTIFY, TA_LEFT, TA_CENTER, TA_RIGHT\n",
    "from reportlab.lib import colors\n",
    "from reportlab.pdfbase.pdfmetrics import registerFont\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NewsletterComposer:\n",
    "    \"\"\"\n",
    "    Erstellt den Newsletter aus den aufbereiteten Daten, typischerweise als PDF.\n",
    "    \"\"\"\n",
    "    def __init__(self, newsletter_data: NewsletterData):\n",
    "        self.newsletter_data = newsletter_data\n",
    "        self.styles = getSampleStyleSheet()\n",
    "        self._register_fonts() \n",
    "        self._define_custom_styles()\n",
    "        logger.info(\"Initialisiere Newsletter Composer.\")\n",
    "\n",
    "    def _register_fonts(self):\n",
    "        # Hier könnten benutzerdefinierte Schriftarten registriert werden (siehe vorheriges Beispiel)\n",
    "        pass \n",
    "\n",
    "    def _define_custom_styles(self):\n",
    "        self.styles.add(ParagraphStyle(name='H1Centered', parent=self.styles['h1'], alignment=TA_CENTER, spaceAfter=0.3*cm, fontSize=18))\n",
    "        self.styles.add(ParagraphStyle(name='H2Left', parent=self.styles['h2'], alignment=TA_LEFT, spaceBefore=0.6*cm, spaceAfter=0.2*cm, textColor=colors.HexColor(\"#003366\"), fontSize=16)) # Dunkelblau\n",
    "        self.styles.add(ParagraphStyle(name='H3Article', parent=self.styles['h3'], alignment=TA_LEFT, spaceAfter=0.1*cm, textColor=colors.HexColor(\"#333333\"), fontSize=14)) # Dunkelgrau\n",
    "        self.styles.add(ParagraphStyle(name='NormalJustified', parent=self.styles['Normal'], alignment=TA_JUSTIFY, spaceAfter=0.2*cm, leading=14, fontSize=10)) # Zeilenabstand, Schriftgröße\n",
    "        self.styles.add(ParagraphStyle(name='ItalicSmall', parent=self.styles['Italic'], fontSize=8, spaceBefore=0.05*cm, spaceAfter=0.1*cm, textColor=colors.dimgrey))\n",
    "        self.styles.add(ParagraphStyle(name='LinkStyle', parent=self.styles['Normal'], textColor=colors.blue, fontSize=9)) \n",
    "        self.styles.add(ParagraphStyle(name='Footer', parent=self.styles['Normal'], alignment=TA_CENTER, fontSize=8, textColor=colors.grey))\n",
    "        self.styles.add(ParagraphStyle(name='RelevanceScore', parent=self.styles['Normal'], fontSize=8, textColor=colors.darkgreen, alignment=TA_RIGHT, spaceBefore=0.1*cm))\n",
    "\n",
    "\n",
    "    def _add_header_footer(self, canvas, doc):\n",
    "        \"\"\"Fügt Kopf- und Fußzeile zu jeder Seite hinzu.\"\"\"\n",
    "        canvas.saveState()\n",
    "        # Fußzeile\n",
    "        footer_text = f\"Seite {doc.page} - {self.newsletter_data.generation_date.strftime('%d.%m.%Y')}\"\n",
    "        canvas.setFont('Times-Roman', 8)\n",
    "        canvas.setFillColor(colors.grey)\n",
    "        canvas.drawCentredString(doc.width/2.0 + doc.leftMargin, 0.75 * inch, footer_text)\n",
    "        \n",
    "        # Optional: Kopfzeile\n",
    "        # header_text = \"Dein täglicher Newsletter\"\n",
    "        # canvas.setFont('Times-Roman', 9)\n",
    "        # canvas.setFillColor(colors.darkgrey)\n",
    "        # canvas.drawString(doc.leftMargin, doc.height + doc.topMargin - 0.5 * inch, header_text)\n",
    "        canvas.restoreState()\n",
    "\n",
    "    def generate_pdf(self, filename: str = \"newsletter.pdf\") -> str:\n",
    "        logger.info(f\"Starte PDF-Generierung: '{filename}'\")\n",
    "        \n",
    "        output_dir = os.path.dirname(filename)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            logger.info(f\"Ausgabeverzeichnis '{output_dir}' erstellt.\")\n",
    "\n",
    "        doc = SimpleDocTemplate(filename,\n",
    "                                rightMargin=1.5*cm, leftMargin=1.5*cm,\n",
    "                                topMargin=1.8*cm, bottomMargin=1.8*cm) # Margins angepasst\n",
    "        \n",
    "        story: List[Any] = []\n",
    "\n",
    "        newsletter_title_text = self.newsletter_data.title or \"Dein personalisierter Newsletter\"\n",
    "        story.append(Paragraph(newsletter_title_text, self.styles['H1Centered']))\n",
    "        \n",
    "        gen_date_str = self.newsletter_data.generation_date.strftime('%A, %d. %B %Y, %H:%M Uhr (UTC)')\n",
    "        story.append(Paragraph(f\"<i>Erstellt am: {gen_date_str}</i>\", self.styles['ItalicSmall']))\n",
    "        story.append(Spacer(1, 0.6*cm))\n",
    "\n",
    "        for section_idx, section in enumerate(self.newsletter_data.sections):\n",
    "            if not section.items: \n",
    "                logger.debug(f\"Überspringe leere Sektion: '{section.title}'\")\n",
    "                continue\n",
    "\n",
    "            # Optional: Seitenumbruch vor jeder neuen Hauptsektion, außer der ersten\n",
    "            # if section_idx > 0:\n",
    "            #     story.append(PageBreak())\n",
    "\n",
    "            story.append(Paragraph(section.title, self.styles['H2Left']))\n",
    "            story.append(Spacer(1, 0.1*cm))\n",
    "            \n",
    "            for item in section.items:\n",
    "                if isinstance(item, ProcessedArticle):\n",
    "                    story.append(Paragraph(f\"<b>{item.title or 'Kein Titel'}</b>\", self.styles['H3Article']))\n",
    "                    \n",
    "                    meta_info_parts = []\n",
    "                    if item.source_name:\n",
    "                        meta_info_parts.append(f\"Quelle: {item.source_name}\")\n",
    "                    if item.published_at:\n",
    "                        meta_info_parts.append(item.published_at.strftime('%d.%m.%y %H:%M'))\n",
    "                    if meta_info_parts:\n",
    "                        story.append(Paragraph(f\"<i>{' | '.join(meta_info_parts)}</i>\", self.styles['ItalicSmall']))\n",
    "\n",
    "                    story.append(Paragraph(item.summary or \"Keine Zusammenfassung.\", self.styles['NormalJustified']))\n",
    "                    \n",
    "                    if item.relevance_score is not None: # Relevanz-Score anzeigen\n",
    "                         story.append(Paragraph(f\"Relevanz: {item.relevance_score*100:.0f}%\", self.styles['RelevanceScore']))\n",
    "\n",
    "                    if item.url:\n",
    "                        story.append(Paragraph(f\"<a href='{str(item.url)}' color='blue'><u>Artikel online lesen</u></a>\", self.styles['LinkStyle']))\n",
    "                    story.append(Spacer(1, 0.4*cm)) # Etwas mehr Abstand\n",
    "\n",
    "                # ... (Behandlung für Event, Birthday, WeatherInfo wie zuvor, ggf. auch mit Anpassungen) ...\n",
    "                \n",
    "                else: \n",
    "                    story.append(Paragraph(f\"Unbekannter Eintrag: {str(item)[:100]}...\", self.styles['Normal']))\n",
    "                    story.append(Spacer(1, 0.3*cm))\n",
    "            story.append(Spacer(1, 0.6*cm)) \n",
    "\n",
    "        try:\n",
    "            doc.build(story, onFirstPage=self._add_header_footer, onLaterPages=self._add_header_footer)\n",
    "            abs_filepath = os.path.abspath(filename)\n",
    "            logger.info(f\"PDF '{abs_filepath}' erfolgreich erstellt.\")\n",
    "            return abs_filepath \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler beim Erstellen des PDF: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "# src/agents/distributors/google_drive_uploader.py\n",
    "# Agent zum Hochladen von Dateien auf Google Drive.\n",
    "\n",
    "from typing import Optional\n",
    "# Die Google Client Library muss installiert sein: pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
    "from google.oauth2.service_account import Credentials \n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from googleapiclient.errors import HttpError\n",
    "from src.utils.config_loader import get_env_variable \n",
    "import logging\n",
    "import os\n",
    "import json # Für Fehlerdetails\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GoogleDriveUploader:\n",
    "    \"\"\"\n",
    "    Lädt Dateien auf Google Drive hoch.\n",
    "    Verwendet ein Service Account für die Authentifizierung.\n",
    "    \"\"\"\n",
    "    def __init__(self, folder_id: Optional[str] = None):\n",
    "        self.folder_id = folder_id\n",
    "        self.service = None\n",
    "        self._initialize_service() # Rufe Initialisierung im Konstruktor auf\n",
    "        if self.service:\n",
    "             logger.info(f\"Google Drive Uploader initialisiert. Zielordner-ID: '{self.folder_id if self.folder_id else 'Root des Service Accounts'}'.\")\n",
    "        # Keine Fehlermeldung hier, wenn _initialize_service fehlschlägt, da es dort schon loggt.\n",
    "\n",
    "    def _initialize_service(self):\n",
    "        \"\"\"Initialisiert den Google Drive API Service.\"\"\"\n",
    "        try:\n",
    "            credentials_path = get_env_variable(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "            if not credentials_path:\n",
    "                logger.warning(\"Umgebungsvariable 'GOOGLE_APPLICATION_CREDENTIALS' nicht gesetzt. Google Drive Uploads sind deaktiviert.\")\n",
    "                return\n",
    "\n",
    "            if not os.path.exists(credentials_path):\n",
    "                logger.error(f\"Service Account Schlüsseldatei nicht gefunden unter: {credentials_path}. Google Drive Uploads sind deaktiviert.\")\n",
    "                return\n",
    "\n",
    "            scopes = ['https://www.googleapis.com/auth/drive.file'] \n",
    "            \n",
    "            creds = Credentials.from_service_account_file(credentials_path, scopes=scopes)\n",
    "            # cache_discovery=False kann bei Problemen in manchen Umgebungen helfen (z.B. Serverless Functions)\n",
    "            self.service = build('drive', 'v3', credentials=creds, cache_discovery=False) \n",
    "            logger.info(\"Google Drive Service erfolgreich initialisiert.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler bei der Initialisierung des Google Drive Service: {e}\", exc_info=True)\n",
    "            self.service = None \n",
    "\n",
    "    def upload_file(self, local_filepath: str, filename_on_drive: str, mimetype: str = 'application/pdf') -> Optional[str]:\n",
    "        if not self.service:\n",
    "            logger.error(\"Google Drive Service nicht initialisiert. Datei-Upload abgebrochen.\")\n",
    "            return None\n",
    "        \n",
    "        if not os.path.exists(local_filepath):\n",
    "            logger.error(f\"Lokale Datei nicht gefunden: '{local_filepath}'. Upload abgebrochen.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            file_metadata = {'name': filename_on_drive}\n",
    "            if self.folder_id:\n",
    "                file_metadata['parents'] = [self.folder_id]\n",
    "            \n",
    "            media = MediaFileUpload(local_filepath, mimetype=mimetype, resumable=True)\n",
    "            \n",
    "            logger.info(f\"Starte Upload von '{local_filepath}' als '{filename_on_drive}' nach Google Drive...\")\n",
    "            \n",
    "            request = self.service.files().create(\n",
    "                body=file_metadata,\n",
    "                media_body=media,\n",
    "                fields='id, name, webViewLink' \n",
    "            )\n",
    "            \n",
    "            file_resource = request.execute()\n",
    "            \n",
    "            file_id = file_resource.get('id')\n",
    "            file_name = file_resource.get('name')\n",
    "            web_view_link = file_resource.get('webViewLink')\n",
    "\n",
    "            logger.info(f\"Datei '{file_name}' erfolgreich auf Google Drive hochgeladen. ID: {file_id}, Link: {web_view_link}\")\n",
    "            return web_view_link\n",
    "\n",
    "        except HttpError as error:\n",
    "            error_reason = \"Unbekannter Grund\"\n",
    "            error_details_str = \"\"\n",
    "            try:\n",
    "                # Versuche, Details aus dem Fehlercontent zu extrahieren\n",
    "                error_content = error.content.decode('utf-8')\n",
    "                error_details = json.loads(error_content)\n",
    "                error_reason = error_details.get('error', {}).get('message', error._get_reason())\n",
    "                error_details_str = f\" Fehlerdetails: {error_details}\"\n",
    "            except Exception: # Falls das Parsen fehlschlägt\n",
    "                error_reason = error._get_reason()\n",
    "                error_details_str = f\" Roh-Fehlercontent: {error.content.decode() if error.content else 'Kein Content'}\"\n",
    "\n",
    "            logger.error(f\"Ein HTTP-Fehler ist beim Google Drive Upload aufgetreten: {error.resp.status} - {error_reason}.{error_details_str}\", exc_info=False)\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ein unerwarteter Fehler ist beim Google Drive Upload aufgetreten: {e}\", exc_info=True)\n",
    "            return None\n",
    "```text\n",
    "# .env.example\n",
    "# Kopiere diese Datei zu .env und fülle deine API-Schlüssel und Konfigurationen ein.\n",
    "# Füge .env zu deiner .gitignore Datei hinzu, um Keys nicht in Git zu committen!\n",
    "\n",
    "# NewsAPI\n",
    "NEWSAPI_API_KEY=\"DEIN_NEWSAPI_SCHLÜSSEL_HIER\"\n",
    "\n",
    "# Google Gemini API (Vertex AI oder AI Studio)\n",
    "GOOGLE_API_KEY=\"DEIN_GOOGLE_GEMINI_API_SCHLÜSSEL_HIER\"\n",
    "GEMINI_DEFAULT_MODEL=\"gemini-1.5-flash-latest\" # oder gemini-1.5-pro-latest\n",
    "\n",
    "# LangSmith (optional, aber empfohlen für Tracing)\n",
    "LANGCHAIN_API_KEY=\"DEIN_LANGSMITH_API_SCHLÜSSEL_HIER (ls__...)\"\n",
    "LANGCHAIN_TRACING_V2=\"true\"\n",
    "LANGCHAIN_ENDPOINT=\"[https://api.smith.langchain.com](https://api.smith.langchain.com)\"\n",
    "LANGCHAIN_PROJECT=\"Newsletter-Pipeline-Projekt\" # Wähle einen Projektnamen für LangSmith\n",
    "\n",
    "# Google Cloud Service Account für Google Drive / Calendar etc. (optional, wenn benötigt)\n",
    "# Der Pfad zur JSON-Schlüsseldatei deines Service Accounts.\n",
    "# GOOGLE_APPLICATION_CREDENTIALS=\"/pfad/zu/deiner/service_account_datei.json\"\n",
    "\n",
    "# Google Drive Ordner ID (optional, für Uploads)\n",
    "# GOOGLE_DRIVE_FOLDER_ID=\"DEINE_GOOGLE_DRIVE_ORDNER_ID_HIER\"\n",
    "\n",
    "# OpenWeatherMap API Key (optional, für Wetter-Agent)\n",
    "# OPENWEATHERMAP_API_KEY=\"DEIN_OPENWEATHERMAP_API_SCHLÜSSEL_HIER\"\n",
    "\n",
    "# Logging Konfiguration (optional)\n",
    "LOG_LEVEL=\"INFO\" # DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "# LOG_FILE=\"logs/newsletter_pipeline.log\" # Optional: Pfad zur Log-Datei\n",
    "\n",
    "# Für den AudienceAlignmentAgent\n",
    "NEWSLETTER_USER_PROFILE=\"\"\"\n",
    "Der Leser interessiert sich primär für tiefgehende technische Analysen im Bereich KI,\n",
    "Softwareentwicklung und Open Source, mit Fokus auf den DACH-Raum und Europa.\n",
    "Bevorzugt werden Nachrichten über neue LLM-Modelle, ethische Implikationen von KI,\n",
    "Durchbrüche in der Forschung und relevante wirtschaftliche Entwicklungen im Tech-Sektor.\n",
    "Weniger relevant sind allgemeine Nachrichten, oberflächliche Produktankündigungen oder\n",
    "nicht-technologiebezogene Politik und Gesellschaftsthemen, es sei denn, sie haben einen direkten KI-Bezug.\n",
    "\"\"\"\n",
    "RELEVANCE_THRESHOLD=\"0.6\" # Schwellenwert (0.0-1.0) für Relevanz, um Artikel in den Newsletter aufzunehmen\n",
    "\n",
    "```text\n",
    "# requirements.txt\n",
    "# Liste der Python-Abhängigkeiten für das Projekt.\n",
    "\n",
    "python-dotenv\n",
    "requests\n",
    "# Langchain Kernkomponenten\n",
    "langchain>=0.1.0 # Stelle sicher, dass eine aktuelle Version verwendet wird\n",
    "langchain-core>=0.1.0\n",
    "# Spezifische Langchain Integrationen\n",
    "langchain-google-genai # Für Gemini\n",
    "# langchain-openai # Falls du OpenAI als Alternative oder für andere Zwecke nutzen willst\n",
    "langsmith # Für Tracing und Monitoring mit LangSmith\n",
    "\n",
    "# Datenvalidierung und -modellierung\n",
    "pydantic\n",
    "\n",
    "# Für RSS Feeds (optional, falls benötigt)\n",
    "feedparser\n",
    "\n",
    "# Für einfaches HTML-Parsing (optional, falls benötigt)\n",
    "beautifulsoup4 \n",
    "\n",
    "# Google Client Libraries (optional, für Google Drive, Calendar etc.)\n",
    "google-api-python-client\n",
    "google-auth-httplib2\n",
    "google-auth-oauthlib\n",
    "\n",
    "# PDF-Generierung\n",
    "reportlab # Eine mächtige Bibliothek für PDF-Erstellung\n",
    "# Alternativen:\n",
    "# fpdf2\n",
    "# weasyprint # Gut für HTML zu PDF Konvertierung (benötigt separate Installation von Pango, Cairo etc.)\n",
    "\n",
    "# Für asynchrone HTTP-Anfragen (optional, für Performance-Optimierung bei vielen API-Calls)\n",
    "# httpx[http2] \n",
    "# asyncio # Teil der Python Standard Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4bfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
